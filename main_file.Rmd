---
title: "R Notebook"
output: html_notebook
---

# **PROJECT 4: Pima Diabetes - Detecting Diabetes from Imperfect Medical Data**

Project Goal : As data analysts in the healthcare sector, your team must build a model to predict the likelihood of diabetes in female patients of Pima Indian heritage. The primary challenge of this dataset is the presence of "impossible" data (zero values for biological measurements) that mask missing values. Your success will depend on your ability to identify and correct these anomalies before modeling.

```{r, include=FALSE}
# echo false pour toutes les cellules
knitr::opts_chunk$set(echo = FALSE) 

# Deleting previous environment
rm(list = ls())

# Importing needed libraries
library(dplyr)
library(ggplot2)
library(gridExtra)  # pour arranger les graphes
library(GGally)
library(caret)
library(class)
```

## **Step 1: Data Cleaning and Preparation**

First, we import the data. Then we look at the beginning of the table to familiarize ourselves with it. And we create a summary to get an overview of the range of numerical values.

```{r}
df = read.csv('diabetes.csv')
head(df)
summary(df) 
```

We observe that certain values are 0. However, this does not always make sense in a medical context. We deduce that these values are missing. We will replace them with ‘NA’.

```{r}
cols_with_zeros = c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")

# Remplacer les zéros par NA dans ces colonnes
df = df %>%
  mutate(across(all_of(cols_with_zeros), ~na_if(., 0)))

head(df)
```

The ‘NA’ value count:

```{r}
colSums(is.na(df))
```

In order to analyze our dataset, we need to fill in the ‘NA’ values. We choose to replace them with the median of their respective column.

```{r}
for (col in cols_with_zeros) {
  median_value = median(df[[col]], na.rm = TRUE)
  df[[col]][is.na(df[[col]])] = median_value
}

head(df)
```

All ‘NA’ values have been replaced & no more 0 remains:

```{r}
colSums(is.na(df))
cat("\n") # Ajout d'une ligne vide
sapply(df[cols_with_zeros], function(x) sum(x == 0))
```

## **Step 2: Feature Transformation and Engineering**

Before starting the analysis, we need to perform some transformations on our data.

### **Part 1: Creating BMI Categories**

Create a new categorical variable BMICategory from BMI using the World Health Organization's standard thresholds.

```{r}
#standard WHO thresholds: underweight, normal weight, overweight, obese
df <- df %>% 
  mutate(BMICategory = case_when(BMI<18.5 ~ 'Underweight', 
                                 BMI>=18.5 & BMI<25 ~ 'Normal weight', 
                                 BMI>=25 & BMI<30 ~ 'Overweight', BMI>=30 ~ 'Obese'),
         BMICategory = factor(BMICategory, levels = c('Underweight', 'Normal weight', 'Overweight',
                                                      'Obese')))
df %>% count(BMICategory)
```

### **Part 2: Creating Blood Pressure Categories**

Similarly, we create a BPCategory variable from BloodPressure

```{r}
#blood pressure categories: low, normal, elevated, high 
df <- df %>% 
  mutate(BPCategory = case_when(BloodPressure < 60 ~ "Low", 
                                BloodPressure >= 60 & BloodPressure < 80 ~ "Normal",
                                BloodPressure >= 80 & BloodPressure < 90 ~ "Elevated", 
                                BloodPressure >= 90 ~ "High"),
         BPCategory = factor(BPCategory, levels = c("Low", "Normal", "Elevated", "High")))

df %>% count(BPCategory)
```

### **Part 3: Target Transformation**

Now, we ensure the target variable Outcome is a factor with clear labels (e.g., "Diabetic", "Not Diabetic") for interpretability.

```{r}
df <- df %>% mutate(OutcomeLabel = case_when(Outcome==0 ~ 'Not Diabetic', 
                                                 Outcome==1 ~ 'Diabetic'),
                    OutcomeLabel = factor(OutcomeLabel, levels = c("Not Diabetic", "Diabetic")))
df %>% count(OutcomeLabel)
```

### **Part 4: Standardizing Variables**

Center and scale all numeric columns so their mean becomes 0 and their standard deviation becomes 1.

```{r}
numeric_cols <- names(df)[sapply(df, is.numeric)]
numeric_cols <- setdiff(numeric_cols, "Outcome")  # on ne touche pas à la cible

df_scaled <- df %>%
  mutate(across(all_of(numeric_cols), scale))

head(df_scaled)
```

## **Step 3: Visualization and Exploration (with ggplot2)**

Now that our data has been cleaned, we can begin the analysis.
Let's first look at the impact of cleaning on glucose value distributions.

```{r}
# Recharger les données brutes pour comparaison
df_original <- read.csv("diabetes.csv")

# Graphe avant nettoyage
p1 <- ggplot(df_original, aes(x = Glucose)) +
  geom_density(fill = "red", alpha = 0.5) +
  labs(title = "Avant nettoyage", x = "Taux de Glucose", y = "Densité") +
  theme_minimal()
 
# Graphe après nettoyage
p2 <- ggplot(df_scaled, aes(x = Glucose)) +
  geom_density(fill = "green", alpha = 0.5) +
  labs(title = "Après nettoyage", x = "Taux de Glucose", y = "Densité") +
  theme_minimal()
 
# Afficher les deux côte à côte
grid.arrange(p1, p2, ncol = 2)
```

### **Part 1: Comparing Distributions**

Now let's look at the distribution of BMI and age according to whether or not the patient has diabetes.

```{r}
#distribution of BMI and age between diabetic and non-diabetic patients 

plot_bmi <- ggplot(df_scaled, aes(x=BMI, fill=OutcomeLabel)) + labs(title = 'BMI distribution', x='BMI', fill='Outcome') + geom_density(alpha = 0.5) 
  
plot_age <- ggplot(df_scaled, aes(x=Age, fill=OutcomeLabel)) + labs(title = 'Age distribution', x='Age', fill='Outcome') + geom_density(alpha = 0.5)

plot_bmi
plot_age
```

It can be observed that diabetes is associated with a higher BMI than among non-diabetic patients.


### **Part 2: Correlation Between Predictors**

Here we calculate the correlation matrix between all numerical variables to identify those that are correlated.

```{r}
numeric_cols <- names(df_scaled)[sapply(df_scaled, is.numeric)]
corr_data <- df_scaled[, numeric_cols]

ggcorr(corr_data, label = TRUE, label_round = 2, hjust = 1, size = 3) +
  coord_cartesian(clip = "off") +                       # ← permet d’afficher hors panel
  ggtitle("Matrice de corrélation entre les variables numériques") +
  theme(plot.margin = margin(20, 100, 20, 100)) # augmente les marges (top, right, bottom, left
```

There is a strong correlation between diabetes rates and glucose levels. As expected.


### **Part 3: Analyzing Pedigree**

On the contrary, if we visualize two variables that have a low correlation score, we cannot distinguish anything in particular.

```{r}
ggplot(df_scaled, aes(x = Age, y = DiabetesPedigreeFunction, color = OutcomeLabel)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(
    title = "Lien entre l'âge et la fonction héréditaire du diabète",
    x = "Âge (standardisé)", y = "Diabetes Pedigree Function") 
```

### **Part 4: Visualisations of our choice**

As announced, glucose distribution is higher in diabetic patients.

```{r}
plot_glucose <- ggplot(df_scaled, aes(x=Glucose, fill=OutcomeLabel)) + labs(title = 'Glucose distribution', x='glucose', fill='Outcome') + geom_density(alpha = 0.5) 
plot_glucose
```

Another thing that was to be expected: blood pressure increases with age.

```{r}
ggplot(df_scaled, aes(x = BloodPressure, y = Age , color = OutcomeLabel)) +
  geom_point(alpha = 0.7, size = 3)
```

## **Step 4:  Modeling and Prediction (Data Science)**

### **Part 1: Logistic Regression Model**

We train a logistic regression model (A model that predicts the probability of a binary event occurring) on the standardized data.
The training data volume represents 80% of the total data. The model is tested on the remaining 20%.

```{r}
set.seed(123)
train_index <- sample(1:nrow(df_scaled), 0.8 * nrow(df_scaled))
train_data <- df_scaled[train_index, ]
test_data <- df_scaled[-train_index, ]

model <- glm(Outcome ~ Glucose + BMI + Age + DiabetesPedigreeFunction + Pregnancies + BloodPressure + SkinThickness, data = train_data, family = binomial)

test_data$PredictedProb <- predict(model, newdata = test_data, type = "response")
test_data$PredictedClass <- ifelse(test_data$PredictedProb > 0.5, 1, 0)

confusion_matrix <- table(Predicted = test_data$PredictedClass, Actual = test_data$Outcome)
accuracy <- mean(test_data$PredictedClass == test_data$Outcome)

confusion_matrix

formula <- (89+29)/(89+23+13+29)
cat("\n")
formula
```

This model allow you to see the effect (positive or negative) of each variable.

```{r}
summary(model)
```

### **Part 2: k-Nearest Neighbors (KNN) Model**

A KNN model classifies a patient based on the majority class of their closest "neighbors" in the feature space.
At first, we take k=3.

```{r}
set.seed(123)
train_index <- sample(1:nrow(df_scaled), 0.8 * nrow(df_scaled))
train_data <- df_scaled[train_index, ]
test_data <- df_scaled[-train_index, ]

train_X <- train_data[, c("Glucose", "BMI", "Age", "DiabetesPedigreeFunction", "Pregnancies", "BloodPressure", "SkinThickness")]
test_X <- test_data[, c("Glucose", "BMI", "Age", "DiabetesPedigreeFunction", "Pregnancies", "BloodPressure", "SkinThickness")]

train_Y <- as.factor(train_data$Outcome)
test_Y <- as.factor(test_data$Outcome)

k <- 3
model_knn_predicted <- knn(train = train_X, test = test_X, cl = train_Y, k = k)

confusion_matrix_knn <- table(Predicted = model_knn_predicted, Actual = test_Y)
confusion_matrix_knn 

cat("\n")
accuracy_knn <- mean(model_knn_predicted == test_Y)
accuracy_knn
```

### **Part 3: Cross-Validation**

Cross-validation involves dividing your dataset into several subsets to train your model on one part and test it on the others, in order to evaluate its performance more reliably and avoid overfitting.

This second version of the LRM offers greater precision than the previous one.

```{r}
df_scaled$Outcome <- as.factor(df_scaled$Outcome)

training <- trainControl(method='cv',number=10)

set.seed(123)

#training of the two models again 

model_TWO <- train(Outcome ~ Glucose + BMI + Age + DiabetesPedigreeFunction + Pregnancies + BloodPressure + SkinThickness, data = df_scaled, method = "glm", family = "binomial", trControl = training)

model_TWO_knn <- train(Outcome ~ Glucose + BMI + Age + DiabetesPedigreeFunction + Pregnancies + BloodPressure + SkinThickness, data = df_scaled, method = "knn", trControl = training, tuneLength = 10)

model_TWO$results

```

The plot shows us that: the higher the k the more accurate is the model.

```{r}
plot(model_TWO_knn)
```


### **Part 4: Analyzing the Impact of Cleaning**

???

```{r}
df = read.csv('diabetes.csv')

set.seed(123)
train_index_zeros <- sample(1:nrow(df), 0.8 * nrow(df))
train_data_zeros <- df[train_index_zeros, ]
test_data_zeros <- df[-train_index_zeros, ]

model_with_zeros <- glm(Outcome ~ Glucose + BMI + Age + DiabetesPedigreeFunction + Pregnancies + BloodPressure + SkinThickness, data = train_data_zeros, family = binomial)

test_data_zeros$PredictedProb <- predict(model_with_zeros, newdata = test_data_zeros, type = "response")
test_data_zeros$PredictedClass <- ifelse(test_data_zeros$PredictedProb > 0.5, 1, 0)

confusion_matrix_zeros <- table(Predicted = test_data_zeros$PredictedClass, Actual = test_data_zeros$Outcome)

confusion_matrix_zeros

cat("\n")
accuracy_zeros <- mean(test_data_zeros$PredictedClass == test_data_zeros$Outcome)
accuracy_zeros

```

